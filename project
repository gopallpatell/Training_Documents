#############################################################################  master of master ###########################################################################################


1) java
2) maven
3) git
4) jenkins
5) ansible
6) docker 
7) terraform



##########for java#########



sudo -i

apt update -y

(before this we need to set the environment variable) 
apt install git -y





##########for maven#########



sudo -i

apt update -y

apt install maven -y




########## for git #########



sudo -i

apt update -y

(before this we need to set the environment variable) 
apt install openjdk-17-jre -y




########## for jenkins #########



 								
#Install Jenkins:

sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \
  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key

echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null

sudo apt-get update

sudo apt-get install jenkins -y

jenkins --version 

systemctl status jenkins 
systemctl stop jenkins 
systemctl start jenkins 
systemctl restart jenkins 
systemctl enable jenkins 

http://<Public_IP_Address>:8080/

E.g.: http://13.127.8.53:8080/

cat /var/lib/jenkins/secrets/initialAdminPassword






######## for ansible ########





sudo -i

sudo apt update -y

sudo apt install software-properties-common -y
sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt update -y
sudo apt install ansible -y

ansible --version

#go to /etc/ansible

#host - Default inventory file
#config
#roles 

#Add User in Ansible Controller : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

su - devopsadmin	


ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				

ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#Goto Node1&2, within ansibleadmin home directory, create .ssh directory

vi authorized_keys

#paste the id_ecdsa.pub of devopsadmin user from controller machine to authorized_keys file in Ansible Node1 

chmod 600 /home/ansibleadmin/.ssh/*


# Use the private IP addr. of AN1 and AN2

ssh ansibleadmin@172.31.34.144
ssh ansibleadmin@172.31.45.232

chown -R devopsadmin:devopsadmin /etc/ansible

#chmod 600 /home/ansibleadmin/.ssh/*


###update vi etc/ansible/hosts


[testnodes]
samplenode1 ansible_ssh_host=172.31.34.144 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.45.232 ansible_ssh_user=ansibleadmin


Eg.::: 

samplenode3 ansible_ssh_host=172.31.38.135 ansible_ssh_user=ansibleadmin
samplenode4 ansible_ssh_host=172.31.38.39 ansible_ssh_user=ansibleadmin


ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping 

ansible devnodes -m ping 






######## for docker ########


 Install Docker Engine 

			
			- Launch Ubuntu Machine - v22.04 
			
			- sudo -i 
			- apt update -y 			
			- apt install docker.io -y



#####add Jenkins (user) to the sudo group

sudo usermod -aG sudo jenkins

sudo systemctl restart jenkins

#####add docker to jenkins group

sudo usermod -aG jenkins docker

sudo systemctl restart jenkins

reboot instance

 usermod -aG docker ansibleadmin



		

#############################################################################  jenkins slave node  ###########################################################################################


#Configure Slave Node1 for Java Maven App. :


sudo -i

apt update -y 

Install Java ::

sudo apt update -y 
sudo apt install openjdk-17-jre -y
java -version						

Install GIT :

sudo apt install git -y

Install Maven - Build Tool :

sudo apt install maven -y 

Create User in Jenkins Slave Machine & Create SSH Keys 

SSH Keys --> is composed of public and private keys 

#Add User : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

su - devopsadmin

ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				


ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#cat id_rsa.pub > authorized_keys

cat id_ecdsa.pub > authorized_keys

chmod 600 /home/devopsadmin/.ssh/*

	

Login to Jenkins - Manage Jenkins - Attach the Slave Node to jenkins Master

	Goto to Manage Jenkins 
		Select Nodes 
			On Nodes Dashboard, Click on New Node
				Give Node Name, and choose permanent agent.




#############################################################################  for kubernetes  ###########################################################################################


#Allow All Traffic for Demo!

#Master Nodes :
####  6443, 2379-2380, 10250, 10259, 10257

Worker Nodes :
####  10250, 30000-32767(NodePort Range)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i

apt update -y

#Set the appropriate hostname for each machine.
#-----------------------------------------------
sudo hostnamectl set-hostname "kmaster-node"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node1"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node2"
exec bash

reboot


#To allow kubelet to work properly, we need to disable swap on both machines.

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

#Install Docker :

apt install docker.io -y

#Step 1. Install containerd			Is a CRI 
#To install containerd, follow these steps on both VMs:

#Load the br_netfilter module required for networking.

sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

#To allow iptables to see bridged traffic, as required by Kubernetes, we need to set the values of certain fields to 1.

sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

#Apply the new settings without restarting.

sudo sysctl --system

#Install curl.

sudo apt install curl -y	

#Get the apt-key and then add the repository from which we will install containerd.

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#Update and then install the containerd package.

sudo apt update -y 
sudo apt install -y containerd.io

#Set up the default configuration file.

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

#Next up, we need to modify the containerd configuration file and ensure that the cgroupDriver is set to systemd. To do so, #edit the following file:

sudo vi /etc/containerd/config.toml

#Scroll down to the following section:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#And ensure that value of SystemdCgroup is set to true Make sure the contents of your section match the following:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#    BinaryName = ""
#    CriuImagePath = ""
#    CriuPath = ""
#    CriuWorkPath = ""
#    IoGid = 0
#    IoUid = 0
#    NoNewKeyring = false
#    NoPivotRoot = false
#    Root = ""
#    ShimCgroup = ""
##    SystemdCgroup = true

#Finally, to apply these changes, we need to restart containerd.

sudo systemctl restart containerd
sudo systemctl status containerd


#Step 2. Install Kubernetes
#With our container runtime installed and configured, we are ready to install Kubernetes.

#Add the repository key and the repository.

sudo apt-get update

# apt-transport-https may be a dummy package; if so, you can skip that package

sudo apt-get install -y apt-transport-https ca-certificates curl gpg


# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
#mkdir /etc/apt/keyrings

sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

sudo apt-get install -y kubelet kubeadm kubectl

#Finally, enable the kubelet service on both systems so we can start it.

sudo systemctl enable kubelet

#######################################################################################################



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Master Node:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Step 3. Setting up the cluster
#With our container runtime and Kubernetes modules installed, we are ready to initialize our Kubernetes cluster.

#Run the following command on the master node to allow Kubernetes to fetch the required images before cluster initialization:

sudo kubeadm config images pull

#Initialize the cluster

#sudo kubeadm init --pod-network-cidr=10.244.0.0/16

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem

#The initialization may take a few moments to finish. Expect an output similar to the following:

#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

#root : visudo 

#su - devopsadmin 

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:

#export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster. Run kubectl apply -f [podnetwork].yaml with one of the options listed at Kubernetes.

#Then you can join any number of worker nodes by running the following on each as root:

#kubeadm join 102.130.122.60:6443 --token s3v1c6.dgufsxikpbn9kflf \
        --discovery-token-ca-cert-hash sha256:b8c63b1aba43ba228c9eb63133df81632a07dc780a92ae2bc5ae101ada623e00

#You will see a kubeadm join at the end of the output. Copy and save it in some file. We will have to run this command on the worker node to allow it to join the cluster. If you forget to save it, or misplace it, you can also regenerate it using this command:

#sudo kubeadm token create --print-join-command

#Now create a folder to house the Kubernetes configuration in the home directory. We also need to set up the required permissions for the directory, and export the KUBECONFIG variable.
#mkdir -p $HOME/.kube
#sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#sudo chown $(id -u):$(id -g) $HOME/.kube/config
#export KUBECONFIG=/etc/kubernetes/admin.conf

#Deploy a pod network to our cluster. This is required to interconnect the different Kubernetes components.

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml


#Expect an output like this:

#podsecuritypolicy.policy/psp.flannel.unprivileged created
#clusterrole.rbac.authorization.k8s.io/flannel created
#clusterrolebinding.rbac.authorization.k8s.io/flannel created
#serviceaccount/flannel created
#configmap/kube-flannel-cfg created
#daemonset.apps/kube-flannel-ds created

#Use the get nodes command to verify that our master node is ready.

kubectl get nodes

#Also check whether all the default pods are running:

kubectl get pods --all-namespaces

#You should get an output like this:

NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-dxhvf              1/1     Running   0          10m
kube-system   coredns-6d4b75cb6d-nkmj4              1/1     Running   0          10m
kube-system   etcd-master-node                      1/1     Running   0          11m
kube-system   kube-apiserver-master-node            1/1     Running   0          11m
kube-system   kube-controller-manager-master-node   1/1     Running   0          11m
kube-system   kube-flannel-ds-jxbvx                 1/1     Running   0          6m35s
kube-system   kube-proxy-mhfqh                      1/1     Running   0          10m
kube-system   kube-scheduler-master-node            1/1     Running   0          11m

#We are now ready to move to the worker node. Execute the kubeadm join from step 2 on the worker node. You should see an output similar to the following:

#This node has joined the cluster:
#* Certificate signing request was sent to apiserver and a response was received.
#* The Kubelet was informed of the new secure connection details.
#Run kubectl get nodes on the control-plane to see this node join the cluster.

#If you go back to the master node, you should now be able to see the worker node in the output of the get nodes command.

kubectl get nodes

#And the output should look as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   18m40s   v1.24.2
worker-node   Ready    <none>                 3m2s     v1.24.2

#Finally, to set the proper role for the worker node, run this command on the master node:

#kubectl label node worker-node node-role.kubernetes.io/worker=worker

#To verify that the role was set:

kubectl get nodes

#The output should show the worker node’s role as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   5m12s   v1.24.1
worker-node   Ready    worker                 2m55s   v1.24.1


sudo kubeadm token create --print-join-command # To generate New Token along with Kubeadm Join Command.


#Add User : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

su - devopsadmin

#ssh-keygen

#for Ubuntu ::
#ssh-keygen -t rsa -b 2048 -m PEM								#ubuntu 20.04

ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				


ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#cat id_rsa.pub > authorized_keys

cat id_ecdsa.pub > authorized_keys

chmod 600 /home/devopsadmin/.ssh/*

# Install Docker Engine in the Slave Node :
apt install docker.io

usermod -aG docker devopsadmin			


#As a root user edit below file:

$ visudo

#add the below mentioned line in the file and save it.

devopsadmin ALL=(ALL) NOPASSWD: ALL

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config





#############################################################################  for production server  ###########################################################################################



#Allow All Traffic for Demo!

#Master Nodes :
####  6443, 2379-2380, 10250, 10259, 10257

Worker Nodes :
####  10250, 30000-32767(NodePort Range)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i

apt update -y

#Set the appropriate hostname for each machine.
#-----------------------------------------------
sudo hostnamectl set-hostname "kmaster-node"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node1"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node2"
exec bash

reboot


#To allow kubelet to work properly, we need to disable swap on both machines.

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

#Install Docker :

apt install docker.io -y

#Step 1. Install containerd			Is a CRI 
#To install containerd, follow these steps on both VMs:

#Load the br_netfilter module required for networking.

sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

#To allow iptables to see bridged traffic, as required by Kubernetes, we need to set the values of certain fields to 1.

sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

#Apply the new settings without restarting.

sudo sysctl --system

#Install curl.

sudo apt install curl -y	

#Get the apt-key and then add the repository from which we will install containerd.

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#Update and then install the containerd package.

sudo apt update -y 
sudo apt install -y containerd.io

#Set up the default configuration file.

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

#Next up, we need to modify the containerd configuration file and ensure that the cgroupDriver is set to systemd. To do so, #edit the following file:

sudo vi /etc/containerd/config.toml

#Scroll down to the following section:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#And ensure that value of SystemdCgroup is set to true Make sure the contents of your section match the following:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#    BinaryName = ""
#    CriuImagePath = ""
#    CriuPath = ""
#    CriuWorkPath = ""
#    IoGid = 0
#    IoUid = 0
#    NoNewKeyring = false
#    NoPivotRoot = false
#    Root = ""
#    ShimCgroup = ""
##    SystemdCgroup = true

#Finally, to apply these changes, we need to restart containerd.

sudo systemctl restart containerd
sudo systemctl status containerd


#Step 2. Install Kubernetes
#With our container runtime installed and configured, we are ready to install Kubernetes.

#Add the repository key and the repository.

sudo apt-get update

# apt-transport-https may be a dummy package; if so, you can skip that package

sudo apt-get install -y apt-transport-https ca-certificates curl gpg


# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
#mkdir /etc/apt/keyrings

sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

sudo apt-get install -y kubelet kubeadm kubectl

#Finally, enable the kubelet service on both systems so we can start it.

sudo systemctl enable kubelet


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

kubeadm join 172.31.8.101:6443 --token 7nd74g.qf0o7ntrb9im3hqr \
        --discovery-token-ca-cert-hash sha256:20a690f48ba814666bf93da0a3460e53b46992e339a515bc2dd4f4a1bbf5d4b1









create server 3 - name it as monitoring server.

1) install prom and grafana
2) take the metrices from the master server(jenkins) and prod server. (install node exporter in both servers.)





















